#本コード_1
import urllib.request #ネットからファイルをダウンロードする標準ライブラリ
import os
import json #ndjsonを辞書へ変換するモジュール
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split #学習・評価用データの分割ユーティリティを読み込み

#np.random.seed(42)
#tf.random.set_seed(42)

#SCALE = 255.0 #正規化用
CLASS_NAMES = ["straight","skinny","wide","frea","jogger"]
label_map = {c:i for i,c in enumerate(CLASS_NAMES)} #各クラスをid付け

MIN_POINTS = 50

# ストローク形式の描画データ(strokes)を系列データ(seq)に変換
def convert_strokes_to_seq(strokes):
    seq = []
    prev_x, prev_y = 0, 0 #seq(前の点の座標)初期化
    for stroke in strokes: #[[x1, x2...], [y1, y2...]]の形
        x_list, y_list = stroke[:2] #strokeを[x, y]の2つに分ける

        for i in range(len(x_list)):
            x, y = x_list[i], y_list[i] #現在の(x,y)座標を取得
            dx, dy = x - prev_x, y - prev_y #前の点との座標差分(Δx,Δy)

             #ペンの状態をone-hotで、
            pen = [1, 0, 0] #ペンを動かしている
            if i == len(x_list) - 1: #最後の点か判定(最終点->ペンを離す状態に切り替え)
                pen = [0, 1, 0]  #ペンを離す
            seq.append([dx, dy] + pen) #差分とペン状態をまとめて1つのベクトルとしてseqに追加
            prev_x, prev_y = x, y #今の点を次の点に設定

    seq.append([0, 0, 0, 0, 1])  #終了を示すトークン
    return seq #完成した系列を返

# ndjsonを読み込む
def load_drawings(folder_path, label=0, limit=2000):
    data = []
    files = sorted(glob.glob(f"{folder_path}/*.ndjson"))  #フォルダ内のndjsonファイルをとる

    for file in files:
        with open(file, "r") as f:
            for i, line in enumerate(f):
                if i >= limit:
                    break
                sample = json.loads(line)  # 各行(json)を辞書に変換
                full_seq = sample["drawing"]
                if len(full_seq) < MIN_POINTS:
                    continue

                # 途中スケッチを生成
                for rate in [0.2, 0.4, 0.6, 0.8, 1.0]:
                    num_points = int(len(full_seq) * rate)
                    if num_points < 1:
                        continue
                    partial_seq = full_seq[:num_points]  # 先頭からnum_pointsまでを取る
                    data.append((partial_seq, label))

    return data

# ファイル読み込み
straight_data = load_drawings("/content/pants_data/straight/straight", label=0)
skinny_data = load_drawings("/content/pants_data/skinny/skinny", label=1)
wide_data = load_drawings("/content/pants_data/wide/wide", label=2)
frea_data = load_drawings("/content/pants_data/frea/frea", label=3)
jogger_data = load_drawings("/content/pants_data/jogger/jogger", label=4)

all_data = straight_data + skinny_data + wide_data + frea_data + jogger_data

# パディングと分割
MAXLEN = 200
#１系列の最長を200(それ以上は切る、足りない部分は0パディング)
X = [seq[:MAXLEN] + [[0]*5]*(MAXLEN-len(seq)) if len(seq)<MAXLEN else seq[:MAXLEN] for seq, _ in all_data]
y = [label for _, label in all_data] #各サンプルのラベルだけを抽出してリスト

# numpy配列に変換
X = np.array(X, dtype=np.float32)# (N, 200, 5)
y = np.array(y, dtype=np.int32)# (N,)

# 8:2に分割)(学習用、テスト用データ)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# モデルの構築
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(MAXLEN, 5)),#200×5
    tf.keras.layers.LSTM(128, return_sequences=True),  # 1層目
    tf.keras.layers.Dropout(0.3),  
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])

model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# モデル学習を開始
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)

# 評価
loss, acc = model.evaluate(X_test, y_test)
print("Accuracy =", acc)


# 結果をファイルに保存
with open("result_log.txt", "w") as f:
    f.write(f"Loss: {loss:.4f}\n")
    f.write(f"Accuracy: {acc:.4f}\n")

#学習したモデルを保存
model.save("/content/qd_apparel_model.keras")

