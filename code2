import os
import json
import numpy as np
import tensorflow as tf
from keras.layers import Bidirectional
from sklearn.model_selection import train_test_split 

#np.random.seed(42)
#tf.random.set_seed(42)

CLASS_NAMES = ["straight","skinny","wide","frea","jogger"]
#各クラスをid付け
label_map = {}
for i, name in enumerate(CLASS_NAMES):
  label_map[name] = i

MIN_POINTS = 50

#左上詰め、スケーリング
def simplified(strokes):
    x_list = []
    y_list = []
    for stroke in strokes:
      x_list += stroke[0]
      y_list += stroke[1]

    min_x, min_y = min(x_list), min(y_list)
    max_x, max_y = max(x_list), max(y_list)
    w = max_x - min_x
    h = max_y - min_y
    s = max(w,h)
    scale = 255.0 / s

    out = []#それぞれのストロークを順番に処理
    for stroke in strokes:
      x_list = []
      y_list = []

      #座標を左上に寄せて、255の範囲にそろえる
      for i in range(len(stroke[0])):
          x = (stroke[0][i] - min_x) * scale
          y = (stroke[1][i] - min_y) * scale
          x_list.append(x)
          y_list.append(y)
      out.append([x_list, y_list])
    return out

# ストローク形式の描画データ(strokes)を系列データ(seq)に変換
def convert_strokes_to_seq(strokes):
    seq_5d = []
    prev_x, prev_y = 0, 0
    for stroke in strokes:
        x_list, y_list = stroke[0], stroke[1]

        for i in range(len(x_list)):
            x, y = x_list[i], y_list[i]
            dx, dy = x - prev_x, y - prev_y

            pen = [1, 0, 0]
            if i == len(x_list) - 1:
                pen = [0, 1, 0]
            seq_5d.append([dx, dy] + pen)
            prev_x, prev_y = x, y

    seq_5d.append([0, 0, 0, 0, 1])
    return seq_5d

def make_partials(seq_5d,label,rates=(0.2, 0.4, 0.6, 0.8, 1.0)):
    out = []
    n = len(seq_5d)
    if n < MIN_POINTS:
      return out
    for r in rates:
      k = int(n * r)
      if k < 1:
        continue
      out.append((seq_5d[:k], label))
    return out

def points_to_xy(strokes_points):
    xy_strokes = []
    for stroke in strokes_points:
        if not stroke:
            continue

        x_list, y_list = [], []
        # 各ストローク内の点を一つずつ
        for point in stroke:
            x = float(point[0])
            y = float(point[1])
            x_list.append(x)
            y_list.append(y)
        # xとyのリストを1つのストロークとしてまとめる
        xy_strokes.append([x_list, y_list])
    return xy_strokes

# ndjsonを読み込む
def load_drawings(folder_path, label=0, limit=2000):
    data = []
    files = sorted(glob.glob(f"{folder_path}/*.ndjson"))  #フォルダ内のndjsonファイルをとる

    for file in files:
        with open(file, "r") as f:
            for i, line in enumerate(f):
                if i >= limit:
                    break
                sample = json.loads(line)  # 各行(json)を辞書に変換
                raw_strokes = sample["drawing"]
                xy = points_to_xy(raw_strokes)
                xy_sim = simplified(xy)
                seq_5d = convert_strokes_to_seq(xy_sim)
                if len(seq_5d) < MIN_POINTS:
                    continue
                data.extend(make_partials(seq_5d, label))
    return data

# ファイル読み込み
straight_data = load_drawings("/content/pants_data/xy_STRAIGHT/n_STRAIGHT", label=0)
skinny_data = load_drawings("/content/pants_data/xy_SKINNY/n_SKINNY", label=1)
wide_data = load_drawings("/content/pants_data/xy_WIDE/n_WIDE", label=2)
frea_data = load_drawings("/content/pants_data/xy_FREA/n_FREA", label=3)
jogger_data = load_drawings("/content/pants_data/xy_JOGGER/n_JOGGER", label=4)

all_data = straight_data + skinny_data + wide_data + frea_data + jogger_data

# パディングと分割
MAXLEN = 500
#それ以上は切る、足りない部分は0パディング
X, Y = [], []
for seq_5d,label in all_data:
    if len(seq_5d) < MAXLEN:
      seq_5d += [[0.0]*5]*(MAXLEN - len(seq_5d))
    else:
      seq_5d = seq_5d[:MAXLEN]

    X.append(seq_5d)
    Y.append(label)

# numpy配列に変換
X = np.array(X, dtype=np.float32)# (N, 500, 5)
Y = np.array(Y, dtype=np.int32)

# 8:2に分割)(学習用、テスト用データ)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)

# モデルの構築
model = tf.keras.Sequential([
    tf.keras.Input(shape=(MAXLEN, 5)),
    tf.keras.layers.Masking(mask_value=0.0),
    Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Dropout(0.3),
    Bidirectional(tf.keras.layers.LSTM(128)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])

model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# モデル学習を開始
history = model.fit(
    X_train, Y_train,
    validation_data=(X_test, Y_test),
    epochs=1
    )

# 評価
loss, acc = model.evaluate(X_test, Y_test)
print("Accuracy =", acc)
print("Loss =", loss)


# 結果をファイルに保存
with open("result_log.txt", "w") as f:
    f.write(f"Loss: {loss:.4f}\n")
    f.write(f"Accuracy: {acc:.4f}\n")

#学習したモデルを保存
model.save("/content/qd_apparel_model.keras")


import matplotlib.pyplot as plt
import numpy as np

hist_dict = history.history

epochs = range(1, len(hist_dict['loss']) + 1)

#Lossグラフ 
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)

# 訓練データと検証データの損失をプロット
plt.plot(epochs, hist_dict['loss'], 'b', label='Training Loss')
plt.plot(epochs, hist_dict['val_loss'], 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)


# Accuracyグラフ
plt.subplot(1, 2, 2) 

# 訓練データと検証データの精度をプロット
plt.plot(epochs, hist_dict['accuracy'], 'b', label='Training Accuracy')
plt.plot(epochs, hist_dict['val_accuracy'], 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout() 

plt.show()

print("グラフの生成を完了しました。")
